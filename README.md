A [Transformer](https://arxiv.org/abs/1706.03762) implementation based on [an implementation from Harvard](http://nlp.seas.harvard.edu/2018/04/03/attention.html)


The [Levenshtein transformer](https://arxiv.org/abs/1905.11006) implementation is built on top of the Transformer above, with many elements borrowed from [Fairseq](https://github.com/pytorch/fairseq)
